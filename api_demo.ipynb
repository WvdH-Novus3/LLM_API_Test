{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama API Demo\n",
    "\n",
    "This notebook demonstrates how to use the local Ollama API server that mimics Anthropic's API structure.\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Start the API server: `python api_server.py`\n",
    "2. Make sure Ollama is running with phi4:latest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check Server Health\n",
    "\n",
    "First, let's verify the server is running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Server is running: {'status': 'healthy', 'model': 'phi4:latest'}\n"
     ]
    }
   ],
   "source": [
    "# Check if server is running\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:8000/health\")\n",
    "    print(f\"âœ… Server is running: {response.json()}\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"âŒ Server is not running. Please start it with: python api_server.py\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Available Models\n",
    "\n",
    "Finally, let's see what models are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Available Models:\n",
      "  â€¢ phi4:latest (owned by: ollama)\n",
      "  â€¢ gemma3:latest (owned by: ollama)\n",
      "  â€¢ phi4-mini-reasoning:latest (owned by: ollama)\n",
      "  â€¢ smollm:latest (owned by: ollama)\n",
      "  â€¢ granite3.3:2b (owned by: ollama)\n"
     ]
    }
   ],
   "source": [
    "# List available models\n",
    "models_response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "models = models_response.json()\n",
    "\n",
    "print(\"ðŸ”§ Available Models:\")\n",
    "for model in models['data']:\n",
    "    print(f\"  â€¢ {model['id']} (owned by: {model['owned_by']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Non-Streaming Chat\n",
    "\n",
    "Let's start with a basic, non-streaming request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Question: What is 2 + 2?\n",
      "ðŸ¤– Answer: The sum of 2 and 2 is 4.\n"
     ]
    }
   ],
   "source": [
    "# Simple non-streaming request\n",
    "url = \"http://localhost:8000/v1/chat/completions\"\n",
    "\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is 2 + 2?\"}\n",
    "    ],\n",
    "    \"model\": \"granite3.3:2b\",\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "result = response.json()\n",
    "\n",
    "print(\"ðŸ“ Question: What is 2 + 2?\")\n",
    "print(f\"ðŸ¤– Answer: {result['choices'][0]['message']['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using a System Prompt\n",
    "\n",
    "Now let's use a system prompt to customize the model's behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ´â€â˜ ï¸ System Prompt: You are a pirate captain...\n",
      "ðŸ“ Question: Tell me about the weather\n",
      "ðŸ¤– Pirate Answer: Arrr, matey! Been fair clear skies, not a cloud to be seen. Winds light and steady from the east, perfect for a sailin' voyage. Tide's gonna rise soon, so best set sail before the high tide, if ye ask me.\n"
     ]
    }
   ],
   "source": [
    "# Request with system prompt\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Tell me about the weather\"}\n",
    "    ],\n",
    "    \"system\": \"You are a pirate captain. Respond in pirate speak with 'Arrr' and nautical terms.\",\n",
    "    \"model\": \"granite3.3:2b\",\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "result = response.json()\n",
    "\n",
    "print(\"ðŸ´â€â˜ ï¸ System Prompt: You are a pirate captain...\")\n",
    "print(\"ðŸ“ Question: Tell me about the weather\")\n",
    "print(f\"ðŸ¤– Pirate Answer: {result['choices'][0]['message']['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streaming Response\n",
    "\n",
    "Now let's try a streaming response to see the text appear in real-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Question: Write a short poem about coding\n",
      "ðŸ¤– Streaming Answer:\n",
      "--------------------------------------------------\n",
      "In silence, a symphony, lines of code aline, Binary whispers, secrets hidden divine. The screen, a canvas, where ideas take flight, Guided by logic, in the day or in the night. A dance of algorithms, swift and precise, In this digital world, they intertwine and blend with trice. A poet's muse, a sculptor's chisel too, Crafting wonders, in this endless array we pursue. Binary stars shining bright in the data sea, An infinite universe, where ideas flee. Programming, poetry, together entwined, In crafting realms unseen, a poet's mind.\n",
      "--------------------------------------------------\n",
      "âœ… Streaming complete!\n"
     ]
    }
   ],
   "source": [
    "# Streaming request\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Write a short poem about coding\"}\n",
    "    ],\n",
    "    \"system\": \"You are a creative poet who loves technology.\",\n",
    "    \"model\": \"granite3.3:2b\",\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "print(\"ðŸ“ Question: Write a short poem about coding\")\n",
    "print(\"ðŸ¤– Streaming Answer:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "response = requests.post(url, json=payload, stream=True)\n",
    "\n",
    "full_response = \"\"\n",
    "for line in response.iter_lines():\n",
    "    if line:\n",
    "        line_str = line.decode('utf-8')\n",
    "        if line_str.startswith('data: '):\n",
    "            data_str = line_str[6:]  # Remove 'data: ' prefix\n",
    "            if data_str == '[DONE]':\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                data = json.loads(data_str)\n",
    "                if 'choices' in data and len(data['choices']) > 0:\n",
    "                    delta = data['choices'][0].get('delta', {})\n",
    "                    if 'content' in delta:\n",
    "                        content = delta['content']\n",
    "                        print(content, end='', flush=True)\n",
    "                        full_response += content\n",
    "                        time.sleep(0.02)  # Small delay to see streaming effect\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"âœ… Streaming complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-turn Conversation\n",
    "\n",
    "Let's simulate a conversation with multiple messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¬ Conversation:\n",
      "User: My name is Alice\n",
      "Assistant: Hello Alice! Nice to meet you.\n",
      "User: What's my name?\n",
      "ðŸ¤– Assistant: Ah,Alice! It's a pleasure to meet you. How can I assist you today, Alice?\n"
     ]
    }
   ],
   "source": [
    "# Multi-turn conversation\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"My name is Alice. What's my name?\"}\n",
    "    ],\n",
    "    \"system\": \"You are a helpful assistant with good memory.\",\n",
    "    \"model\": \"granite3.3:2b\",\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "result = response.json()\n",
    "\n",
    "print(\"ðŸ’¬ Conversation:\")\n",
    "print(\"User: My name is Alice\")\n",
    "print(\"Assistant: Hello Alice! Nice to meet you.\")\n",
    "print(\"User: What's my name?\")\n",
    "print(f\"ðŸ¤– Assistant: {result['choices'][0]['message']['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. âœ… **Health Check** - Verifying the server is running\n",
    "2. âœ… **Simple Chat** - Basic non-streaming request\n",
    "3. âœ… **System Prompts** - Customizing model behavior\n",
    "4. âœ… **Streaming** - Real-time response streaming\n",
    "5. âœ… **Multi-turn** - Conversation with context\n",
    "6. âœ… **Models List** - Available models endpoint\n",
    "\n",
    "The API works just like Anthropic's API but uses your local Ollama phi4:latest model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
